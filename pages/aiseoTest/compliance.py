from io import BytesIO

import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AutoFeatureExtractor, AutoModelForImageClassification,AutoProcessor,BlipProcessor,BlipForConditionalGeneration,CLIPProcessor,CLIPModel
from PIL import Image
import torch
import json
from selenium import webdriver

# åˆ¤æ–­æ–‡ç« æ˜¯å¦åŒ…å«è¿ç¦è¯
def JudgeLllegalWords(text):
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased")
    model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-multilingual-cased", num_labels=2)

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    predicted = torch.argmax(outputs.logits, dim=1).item()
    return predicted

import torchvision.transforms as T
import numpy as np

# ä¸‹è½½å¹¶åŠ è½½å›¾ç‰‡
def load_image(url_or_path):
    response = requests.get(url_or_path, timeout=5)
    if response.status_code != 200:
        print(f"å›¾ç‰‡çŠ¶æ€ç æœ‰é—®é¢˜ï¼š{response.status_code}")
    else :
        print("å›¾ç‰‡çŠ¶æ€ç æ²¡é—®é¢˜")
    if url_or_path.startswith("http"):
        image = Image.open(requests.get(url_or_path, stream=True).raw).convert("RGB")
    else:
        image = Image.open(url_or_path).convert("RGB")
    return image

def checkImage(urlArr):
    for url in urlArr:
        response = requests.get(url, timeout=5)
        if response.status_code != 200:
            print(f"å›¾ç‰‡çŠ¶æ€ç æœ‰é—®é¢˜ï¼š{response.status_code}")
            return False
        else:
            print("å›¾ç‰‡çŠ¶æ€ç æ²¡é—®é¢˜")
    return True

# æ¨¡æ‹Ÿ NSFW å’Œ horror åˆ¤æ–­ï¼ˆéœ€è°ƒç”¨ä½ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
def detect_nsfw(image_tensor):
    # è¿™é‡Œè°ƒç”¨ nsfw_model(image_tensor) è¿”å›æ ‡ç­¾/æ¦‚ç‡
    # åŠ è½½æ¨¡å‹
    processor = AutoProcessor.from_pretrained("Falconsai/nsfw_image_detection")
    model = AutoModelForImageClassification.from_pretrained("Falconsai/nsfw_image_detection")

    # åŠ è½½å›¾ç‰‡
    # è·å–å›¾ç‰‡å†…å®¹ï¼ˆç•™åœ¨å†…å­˜ä¸­ï¼‰
    to_pil = T.ToPILImage()
    image = to_pil(image_tensor.squeeze(0)).convert("RGB")

    # ä½¿ç”¨ PIL æ‰“å¼€å›¾ç‰‡ï¼ˆä¸ä¿å­˜æœ¬åœ°ï¼‰
    inputs = processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)

    # è¾“å‡ºæ ‡ç­¾å’Œæ¦‚ç‡
    labels = model.config.id2label
    arr = []
    for i, p in enumerate(probs[0]):
        arr.append(f"{labels[i]}: {p.item():.2%}")
        print(f"{labels[i]}: {p.item():.2%}")

    return arr

def detect_horror(image_tensor):
    response = requests.get(image_tensor, allow_redirects=True)
    params = {
        'url': response.url,
        'models': 'gore-2.0',
        'api_user': '285381362',
        'api_secret': 'yWPvsrEKjA8SuYynXGbEbvmEW5gAhhNc'
    }
    r = requests.get('https://api.sightengine.com/1.0/check.json', params=params)
    response = json.loads(r.text)
    gore = response["gore"]
    if (gore["prob"] > 0.001) :
        return True
    else:
        return False


# AI é”™å›¾æ£€æµ‹ï¼ˆåŸºç¡€ç‰ˆï¼‰
def detect_anomaly(image):
    import cv2
    import mediapipe as mp
    np_img = np.array(image)
    mp_pose = mp.solutions.pose
    with mp_pose.Pose(static_image_mode=True) as pose:
        results = pose.process(cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR))
        if not results.pose_landmarks:
            return "No human detected"
        keypoints = results.pose_landmarks.landmark
        if len(keypoints) > 33:  # mediapipe æ­£å¸¸æ˜¯33ç‚¹ï¼Œæ›´å¤šå¯èƒ½æ˜¯é‡å¤æˆ–é”™è¯¯è¯†åˆ«
            return "Possibly abnormal (extra limbs)"
    return "Normal"

# ä¸»å‡½æ•°
def run_all(image_path_or_url):
    image = load_image(image_path_or_url)

    # è½¬ä¸ºtensorï¼ˆä¾›å…¶ä»–æ¨¡å‹ä½¿ç”¨ï¼‰
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor()
    ])
    img_tensor = transform(image).unsqueeze(0)

    nsfw_result = detect_nsfw(img_tensor)
    horror_result = detect_horror(image_path_or_url)
    anomaly_result = detect_anomaly(image)

    print("NSFW æ£€æµ‹:", nsfw_result)
    print("ææ€–å›¾æ£€æµ‹:", horror_result)
    print("AI å¼‚å¸¸æ£€æµ‹:", anomaly_result)

# åˆ¤æ–­æ–‡ç« é‡Œé¢çš„å›¾ç‰‡æ˜¯å¦è¿è§„
def judgeLllgalImage(imageArray):
    for image in imageArray:
        run_all(image)

def judgeNSFWImage(imageArray):
    for image in imageArray:
       img_tensor = run_all(image)
       nsfw_result = detect_nsfw(img_tensor)
       for nsfw in nsfw_result:
           key, value = nsfw.split(':')
           if key == 'normal' and value > 50:
               return False
    return True

def judgeHorrorImage(imageArray):
    for image in imageArray:
        if detect_horror(image):
            return False
    return True

def judgeAnomalyImage(imageArray):
    for image in imageArray:
        image = load_image(image)
        if detect_anomaly(image) == 'Possibly abnormal (extra limbs)':
            return False
    return True


# å…³é”®è¯å‡ºç°çš„æ¬¡æ•°ä¸å¯†åº¦

import spacy
from spacy.matcher import PhraseMatcher

# åŠ è½½ spaCy çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
nlp = spacy.load("xx_ent_wiki_sm")  # å°å‹å¤šè¯­è¨€æ¨¡å‹ï¼Œé€‚åˆä¸­æ–‡+è‹±æ–‡æ··åˆ


def matchKeyword(text, keywords):
    doc = nlp(text)

    matcher = PhraseMatcher(nlp.vocab, attr="LOWER")  # ä¸åŒºåˆ†å¤§å°å†™
    patterns = [nlp.make_doc(kw.lower()) for kw in keywords]
    matcher.add("KeywordList", patterns)

    # ç»Ÿè®¡åŒ¹é…
    matches = matcher(doc)
    match_count = {}
    total_words = len([token for token in doc if token.is_alpha])

    for match_id, start, end in matches:
        phrase = doc[start:end].text.lower()
        match_count[phrase] = match_count.get(phrase, 0) + 1

    # è®¡ç®—å¯†åº¦
    density = {}
    for kw in match_count:
        density[kw] = round((match_count[kw] / total_words) * 100, 2)

    print("å…³é”®å­—å¯†åº¦ï¼š", density)
    print("å…³é”®å­—å‡ºç°æ¬¡æ•°ï¼š", match_count)

    # åˆ¤æ–­æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½å‡ºç°ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
    normalized_keywords = set([kw.lower() for kw in keywords])
    matched_keywords = set(match_count.keys())

    if normalized_keywords.issubset(matched_keywords):
        print("âœ… æ‰€æœ‰å…³é”®å­—éƒ½å‡ºç°äº†")
        return True
    else:
        missing = normalized_keywords - matched_keywords
        print("âš ï¸ æœ‰å…³é”®å­—æ²¡æœ‰å‡ºç°ï¼š", missing)
        return False


# æŸ¥çœ‹Hç»“æ„æ˜¯å¦æ­£å¸¸
from selenium.webdriver.common.by import By
def analyze_headings_with_selenium(edge):
    try:
        headings = []
        for level in range(1, 7):
            tags = edge.find_elements(By.TAG_NAME, f'h{level}')
            for tag in tags:
                text = tag.text.strip()
                if text:
                    headings.append((f'h{level}', text))

        h1_count = sum(1 for tag, _ in headings if tag == 'h1')
        if h1_count == 0:
            print("âŒ ç¼ºå°‘ <h1> æ ‡ç­¾")
            return False
        elif h1_count > 1:
            print(f"âš ï¸ å­˜åœ¨å¤šä¸ª <h1> æ ‡ç­¾ï¼ˆ{h1_count} ä¸ªï¼‰")
        else:
            print("âœ… æ­£å¸¸åŒ…å«ä¸€ä¸ª <h1> æ ‡ç­¾")

        print("\nğŸ“š é¡µé¢ H æ ‡ç­¾ç»“æ„ï¼š")
        last_level = 0
        for tag, text in headings:
            level = int(tag[1])
            if last_level and level > last_level + 1:
                print(f"âš ï¸ è·³çº§ç»“æ„ï¼š{tag.upper()} åœ¨ {last_level} åå‡ºç°")
                return False
            print(f"  {tag.upper()}: {text}")
            last_level = level
        return True
    finally:
        print()