import csv
import requests
from PIL import Image
import json
from langdetect import detect
import jieba
from bs4 import BeautifulSoup
import time

class APIRequester:
    def __init__(self):
        with open('../utils/sightengineApikey.csv', 'r') as f:
            reader = csv.reader(f)
            api_keys = [row[0] for row in reader if row]
        self.api_keys = api_keys
        self.index = 0

    def get_current_key(self):
        if self.index >= len(self.api_keys):
            return None
        return self.api_keys[self.index]

    def switch_key(self):
        self.index += 1

    def make_request(self, url, params=None, headers=None):
        retries = 0
        max_retries = len(self.api_keys)
        while retries < max_retries and self.index < len(self.api_keys):
            api_key = self.get_current_key()
            print(f"ä½¿ç”¨apikey: {api_key}")

            # æ›´æ–°headeråŠ ä¸Šapikeyï¼ˆæ ¹æ®ä½ çš„æ¥å£è°ƒæ•´ï¼‰
            if headers is None:
                headers = {}
            headers['Authorization'] = f"Bearer {api_key}"

            api_key_arr = api_key.split('-')
            params['api_user'] = api_key_arr[0]
            params['api_secret'] = api_key_arr[1]

            try:
                if 'text' in params :
                    response = requests.get(url, data=params, headers=headers, timeout=10)
                else :
                    response = requests.get(url, params=params, headers=headers, timeout=10)
                if response.status_code == 200:
                    return response.json()
                else:
                    print(f"âš ï¸ æ¥å£è¿”å›é”™è¯¯ {response.status_code}ï¼Œå°è¯•æ›´æ¢API Key...")
                    self.switch_key()
                    retries += 1
                    time.sleep(1)  # é˜²æ­¢é¢‘ç¹è¯·æ±‚è¢«å°
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}ï¼Œå°è¯•æ›´æ¢API Key...")
                self.switch_key()
                retries += 1
                time.sleep(1)

        raise Exception("âŒ æ‰€æœ‰API Keyéƒ½ä¸å¯ç”¨æˆ–é‡è¯•å¤±è´¥")

def detect_language_langdetect(text):
    try:
        lang = detect(text)
        return lang  # 'zh-cn' è¡¨ç¤ºä¸­æ–‡ï¼Œ'en' è¡¨ç¤ºè‹±æ–‡
    except:
        return "Unknown"

# åŠ è½½å¤–éƒ¨è¿è§„è¯åº“
def load_banned_words(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        banned_words = f.read().splitlines()
    return banned_words

# ä½¿ç”¨ç»“å·´è¿›è¡Œåˆ†è¯æ£€æµ‹
def detect_banned_words(text, banned_words):
    words = jieba.lcut(text)
    found_words = [word for word in words if word in banned_words]

    if found_words:
        return False, found_words
    else:
        return True, []

# åˆ¤æ–­æ–‡ç« æ˜¯å¦åŒ…å«è¿ç¦è¯
def JudgeLllegalWords(text):
    langdetect = detect_language_langdetect(text)
    if langdetect == 'zh-cn':
        # åŠ è½½è¿è§„è¯åº“
        banned_words = load_banned_words('../utils/è‰²æƒ…ç±».txt')
        # æ£€æµ‹æ˜¯å¦æœ‰è¿ç¦è¯
        is_banned, banned_found = detect_banned_words(text, banned_words)
        print("è¿è§„è¯ï¼š", banned_found)
        return is_banned
    else:
        categories = 'profanity,personal,link,drug,weapon,spam,content-trade,money-transaction,extremism,violence,self-harm,medical'
        data = {
            'text': text,
            'mode': 'rules',
            'lang': 'en',
            'categories': categories,
            'api_user': '',
            'api_secret': ''
        }
        try:
            url = 'https://api.sightengine.com/1.0/text/check.json'
            requester = APIRequester()
            result = requester.make_request(url, data)
            output = json.loads(result.text)
            available = categories.split(',')
            for ava in available:
                element = output.get(ava, {})
                matches = element.get('matches', [])
                if len(matches) > 0:
                    for match in matches:
                        if match.get('intensity') != 'low':
                            return False
            return True
        except Exception as e:
            print("è¯·æ±‚æŠ¥é”™ï¼š", e)

import torchvision.transforms as T
import numpy as np

# ä¸‹è½½å¹¶åŠ è½½å›¾ç‰‡
def load_image(url_or_path):
    response = requests.get(url_or_path, timeout=5)
    if response.status_code != 200:
        print(f"å›¾ç‰‡çŠ¶æ€ç æœ‰é—®é¢˜ï¼š{response.status_code}")
    else :
        print("å›¾ç‰‡çŠ¶æ€ç æ²¡é—®é¢˜")
    if url_or_path.startswith("http"):
        image = Image.open(requests.get(url_or_path, stream=True).raw).convert("RGB")
    else:
        image = Image.open(url_or_path).convert("RGB")
    return image


def get_final_url(url):
    try:
        response = requests.get(url, allow_redirects=True)
        return response.url
    except Exception as e:
        return str(e)

def checkImage(urlArr):
    for url in urlArr:
        response = requests.get(url, timeout=5)
        if response.status_code != 200:
            print(f"å›¾ç‰‡çŠ¶æ€ç æœ‰é—®é¢˜ï¼š{response.status_code}")
            return False
        else:
            print("å›¾ç‰‡çŠ¶æ€ç æ²¡é—®é¢˜")
    return True

# æ¨¡æ‹Ÿ NSFW å’Œ horror åˆ¤æ–­ï¼ˆéœ€è°ƒç”¨ä½ è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
def detect_nsfw(image_tensor):
    params = {
        'url': image_tensor,
        'models': 'nudity-2.1',
        'api_user': '285381362',
        'api_secret': 'yWPvsrEKjA8SuYynXGbEbvmEW5gAhhNc'
    }
    url = 'https://api.sightengine.com/1.0/check.json'
    requester = APIRequester()

    try:
        # r = requests.get('https://api.sightengine.com/1.0/check.json', params=params)
        result = requester.make_request(url, params)
        nudity = result.get("nudity", {})

        # å¯ä»¥é€‰æ‹©å…³æ³¨çš„å­—æ®µï¼ˆè¶Šé å‰è¶Šä¸¥é‡ï¼‰
        flags = [
            "sexual_activity",
            "sexual_display",
            "erotica",
            "very_suggestive",
            "suggestive",
            "mildly_suggestive"
        ]

        for flag in flags:
            if nudity.get(flag, 0) > 0.5:
                print(f'è¯¥å›¾ç‰‡{flag}æŒ‡æ•°è¿‡é«˜ï¼')
                return True  # è¿™å¼ å›¾è¢«è®¤ä¸ºæœ‰é»„å†…å®¹

        return False  # å®‰å…¨
    except Exception as e:
        print("è¯·æ±‚æŠ¥é”™ï¼š", e)

def detect_horror(image_tensor):
    requester = APIRequester()
    response = requests.get(image_tensor, allow_redirects=True)
    url = 'https://api.sightengine.com/1.0/check.json'
    params = {
        'url': response.url,
        'models': 'gore-2.0',
        'api_user': '',
        'api_secret': ''
    }
    # r = requests.get('https://api.sightengine.com/1.0/check.json', params=params)
    result = requester.make_request(url, params)
    gore = result["gore"]
    if (gore["prob"] > 0.001) :
        return True
    else:
        return False


# AI é”™å›¾æ£€æµ‹ï¼ˆåŸºç¡€ç‰ˆï¼‰
def detect_anomaly(image):
    import cv2
    import mediapipe as mp
    np_img = np.array(image)
    mp_pose = mp.solutions.pose
    with mp_pose.Pose(static_image_mode=True) as pose:
        results = pose.process(cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR))
        if not results.pose_landmarks:
            return "No human detected"
        keypoints = results.pose_landmarks.landmark
        if len(keypoints) > 33:  # mediapipe æ­£å¸¸æ˜¯33ç‚¹ï¼Œæ›´å¤šå¯èƒ½æ˜¯é‡å¤æˆ–é”™è¯¯è¯†åˆ«
            return "Possibly abnormal (extra limbs)"
    return "Normal"

def judgeNSFWImage(imageArray):
    for image in imageArray:
       image = get_final_url(image)
       nsfw_result = detect_nsfw(image)
       if nsfw_result:
           return False
    return True

def judgeHorrorImage(imageArray):
    for image in imageArray:
        if detect_horror(image):
            return False
    return True

def judgeAnomalyImage(imageArray):
    for image in imageArray:
        image = load_image(image)
        if detect_anomaly(image) == 'Possibly abnormal (extra limbs)':
            return False
    return True


# å…³é”®è¯å‡ºç°çš„æ¬¡æ•°ä¸å¯†åº¦

import spacy
from spacy.matcher import PhraseMatcher

# åŠ è½½ spaCy çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
nlp = spacy.load("xx_ent_wiki_sm")  # å°å‹å¤šè¯­è¨€æ¨¡å‹ï¼Œé€‚åˆä¸­æ–‡+è‹±æ–‡æ··åˆ


def matchKeyword(text, keywords):
    doc = nlp(text)

    matcher = PhraseMatcher(nlp.vocab, attr="LOWER")  # ä¸åŒºåˆ†å¤§å°å†™
    patterns = [nlp.make_doc(kw.lower()) for kw in keywords]
    matcher.add("KeywordList", patterns)

    # ç»Ÿè®¡åŒ¹é…
    matches = matcher(doc)
    match_count = {}
    total_words = len([token for token in doc if token.is_alpha])

    for match_id, start, end in matches:
        phrase = doc[start:end].text.lower()
        match_count[phrase] = match_count.get(phrase, 0) + 1

    # è®¡ç®—å¯†åº¦
    density = {}
    for kw in match_count:
        density[kw] = round((match_count[kw] / total_words) * 100, 2)

    print("å…³é”®å­—å¯†åº¦ï¼š", density)
    print("å…³é”®å­—å‡ºç°æ¬¡æ•°ï¼š", match_count)

    # åˆ¤æ–­æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½å‡ºç°ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
    normalized_keywords = set([kw.lower() for kw in keywords])
    matched_keywords = set(match_count.keys())

    if normalized_keywords.issubset(matched_keywords):
        print("âœ… æ‰€æœ‰å…³é”®å­—éƒ½å‡ºç°äº†")
        return True
    else:
        missing = normalized_keywords - matched_keywords
        print("âš ï¸ æœ‰å…³é”®å­—æ²¡æœ‰å‡ºç°ï¼š", missing)
        return False


# æŸ¥çœ‹Hç»“æ„æ˜¯å¦æ­£å¸¸
from selenium.webdriver.common.by import By
def analyze_headings_with_selenium(edge, cleaned_text):
    try:
        headings = []
        for level in range(1, 7):
            tags = edge.find_elements(By.TAG_NAME, f'h{level}')
            for tag in tags:
                text = tag.text.strip()
                if text:
                    headings.append((f'h{level}', text))

        h1_count = sum(1 for tag, _ in headings if tag == 'h1')
        if h1_count == 0:
            print("âŒ ç¼ºå°‘ <h1> æ ‡ç­¾")
            return False
        elif h1_count > 1:
            print(f"âš ï¸ å­˜åœ¨å¤šä¸ª <h1> æ ‡ç­¾ï¼ˆ{h1_count} ä¸ªï¼‰")
        else:
            print("âœ… æ­£å¸¸åŒ…å«ä¸€ä¸ª <h1> æ ‡ç­¾")

        print("\nğŸ“š é¡µé¢ H æ ‡ç­¾ç»“æ„ï¼š")
        last_level = 0
        for tag, text in headings:
            level = int(tag[1])
            if last_level and level > last_level + 1:
                print(f"âš ï¸ è·³çº§ç»“æ„ï¼š{tag.upper()} åœ¨ {last_level} åå‡ºç°")
                return False
            print(f"  {tag.upper()}: {text}")
            last_level = level
    finally:
        soup = BeautifulSoup(cleaned_text, "html.parser")
        title_tag = soup.find("title")
        title_text = title_tag.text.strip() if title_tag else ""
        length = len(title_text)
        print(f"\n[Meta Title] é•¿åº¦: {length} å­—ç¬¦ï¼Œå†…å®¹ï¼š{title_text}")
        if not (title_tag and 10 <= length <= 60):
            return False
        desc_tag = soup.find("meta", attrs={"name": "description"})
        desc = desc_tag.get("content", "") if desc_tag else ""
        length = len(desc.strip())
        print(f"\n[Meta Description] é•¿åº¦: {length} å­—ç¬¦ï¼Œå†…å®¹ï¼š{desc.strip()}")
        if not (desc_tag is not None and 50 <= length <= 160):
            return False
        return True