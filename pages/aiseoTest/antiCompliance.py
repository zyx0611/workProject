from serpapi import GoogleSearch
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
import requests
from bs4 import BeautifulSoup
import difflib
API_KEY = "cffb95c1bfd214f02a230d7c564892c3ad4b5836fcfb7fefdf2bf7361062b794"

# ä¼ªåŸåˆ›æ£€æµ‹
def check_duplicate_by_search(text, threshold=0.5):
    sentences = [s.strip() for s in text.split("ã€‚") if len(s.strip()) > 5]
    hit_count = 0

    for sent in sentences:
        params = {
            "engine": "google",
            "q": f'"{sent}"',  # ä½¿ç”¨å¼•å·ç²¾ç¡®åŒ¹é…å¥å­
            "api_key": API_KEY
        }

        search = GoogleSearch(params)
        results = search.get_dict()

        if "organic_results" in results and len(results["organic_results"]) > 0:
            hit_count += 1
            print(f"[é‡å¤] å¥å­ï¼š{sent}")
        else:
            print(f"[åŸåˆ›] å¥å­ï¼š{sent}")

        time.sleep(1.5)  # é˜²æ­¢è¶…é€Ÿè¯·æ±‚

    hit_ratio = hit_count / len(sentences)
    print(f"\nç–‘ä¼¼é‡å¤ç‡ï¼š{hit_ratio:.2%}")

    print("\nç»“æœï¼š", "ç–‘ä¼¼æŠ„è¢­" if hit_ratio > threshold else "åŸåˆ›å†…å®¹") # è¶…è¿‡50%å°±è®¤å®šä¸ºé‡å¤
    if hit_ratio > threshold:
        return False
    else:
        return True

# é‡å®šå‘æ¬ºéª—
def Redirect_deception(edge):
    pageSource = edge.page_source

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ meta refresh æ ‡ç­¾
    if '<meta http-equiv="refresh"' in pageSource:
        print("é¡µé¢åŒ…å« Meta Refresh è·³è½¬")
        return False
    else:
        print("æ²¡æœ‰æ‰¾åˆ° Meta Refresh è·³è½¬")

    # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦å­˜åœ¨ `window.location.href` æˆ–ç±»ä¼¼é‡å®šå‘ä»£ç 
    scripts = edge.find_elements(By.TAG_NAME, "script")

    for script in scripts:
        if "window.location.href" in script.get_attribute("innerHTML"):
            print("é¡µé¢åŒ…å« JavaScript é‡å®šå‘")
            return False
        else:
            print("æ²¡æœ‰æ‰¾åˆ° JavaScript é‡å®šå‘")
    return True

# å¤–é“¾ä½œå¼Š
# ä½ çš„ IPQS API Key
IPQS_API_KEY = "uRjeBGd9dFWtBmxnNU78k0k8v7HenuaF"  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„

def check_link_with_ipqs(link):
    """
    ä½¿ç”¨ IPQS æ£€æŸ¥é“¾æ¥æ˜¯å¦ä¸ºå¯ç–‘ç½‘ç«™
    """
    api_url = f"https://ipqualityscore.com/api/json/url/{IPQS_API_KEY}/{link}"
    try:
        response = requests.get(api_url)
        data = response.json()
        return {
            "is_suspicious": data.get("suspicious", False),
            "unsafe": data.get("unsafe", False),
            "category": data.get("category", "unknown"),
            "domain_rank": data.get("domain_rank", 0)
        }
    except Exception as e:
        return {
            "is_suspicious": False,
            "unsafe": False,
            "category": "error",
            "domain_rank": 0
        }

def analyze_links(edge):
    """
    åˆ†æç½‘é¡µä¸­æ‰€æœ‰çš„ a æ ‡ç­¾é“¾æ¥ï¼Œåˆ¤æ–­ nofollow çŠ¶æ€å¹¶æ£€æµ‹é“¾æ¥è´¨é‡
    """

    a_tags = edge.find_elements(By.TAG_NAME, 'a')
    results = []

    for a in a_tags:
        href = a.get_attribute('href')
        rel = a.get_attribute('rel') or ''
        anchor_text = a.text.strip()
        is_nofollow = 'nofollow' in rel.lower()

        if href and href.startswith('http'):
            quality_check = check_link_with_ipqs(href)
            results.append({
                'é“¾æ¥': href,
                'é”šæ–‡æœ¬': anchor_text,
                'nofollow': is_nofollow,
                'å¯ç–‘': quality_check['is_suspicious'],
                'ä¸å®‰å…¨': quality_check['unsafe'],
                'åˆ†ç±»': quality_check['category'],
                'åŸŸåæ’å': quality_check['domain_rank']
            })

    edge.quit()
    return results

# å®šä½æ­£æ–‡å…ƒç´ ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é€‰æ‹©å™¨ï¼‰
# body_element = edge.find_element(By.CLASS_NAME, "markdown-content")

# æå–å¹¶æ¸…ç†æ–‡æœ¬
# raw_text = body_element.get_attribute("innerText")
# cleaned_text = raw_text.replace("\n", " ")  # åˆå¹¶æ¢è¡Œç¬¦

def get_page_content(url, user_agent):
    headers = {'User-Agent': user_agent}
    response = requests.get(url, headers=headers, timeout=10)
    return response.text

def extract_text(html):
    soup = BeautifulSoup(html, 'html.parser')
    # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆä¸å«æ ‡ç­¾ï¼‰
    return soup.get_text(separator=' ', strip=True)

def check_cloaking(url):
    user_agent_bot = "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
    user_agent_user = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"

    html_bot = get_page_content(url, user_agent_bot)
    html_user = get_page_content(url, user_agent_user)

    text_bot = extract_text(html_bot)
    text_user = extract_text(html_user)

    # ä½¿ç”¨ difflib è®¡ç®—å·®å¼‚ç›¸ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, text_bot, text_user).ratio()

    print(f"é¡µé¢æ–‡æœ¬ç›¸ä¼¼åº¦ï¼š{similarity:.2f}")
    if similarity < 0.8:
        print("âš ï¸ å¯èƒ½å­˜åœ¨ cloaking æ¬ºéª—è¡Œä¸ºï¼ˆæœç´¢å¼•æ“ä¸ç”¨æˆ·çœ‹åˆ°å†…å®¹å·®å¼‚å¤§ï¼‰")
        return False
    else:
        print("âœ… é¡µé¢å†…å®¹ä¸€è‡´ï¼Œæ— æ˜æ˜¾ cloaking")
        return True

import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import jieba
import re

def fetch_text(url):
    headers = {'User-Agent': "Mozilla/5.0 Chrome/120"}
    response = requests.get(url, headers=headers, timeout=10)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup.get_text(separator=' ', strip=True)

def tokenize_mixed(text):
    chinese_words = jieba.lcut(text)
    english_words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
    tokens = chinese_words + english_words
    tokens = [w.lower() for w in tokens if len(w.strip()) > 1]
    return tokens

def analyze_keyword_stuffing_with_density(url, tfidf_threshold=0.15, density_threshold=5.0):
    text = fetch_text(url)
    tokens = tokenize_mixed(text)
    total_words = len(tokens)
    word_counts = Counter(tokens)

    # è‡ªå®šä¹‰ tokenizer ç»™ TfidfVectorizer
    def dummy_tokenizer(doc):
        return tokenize_mixed(doc)

    vectorizer = TfidfVectorizer(tokenizer=dummy_tokenizer, stop_words=None, max_features=100)
    tfidf_matrix = vectorizer.fit_transform([text])
    tfidf_values = tfidf_matrix.toarray()[0]
    tfidf_words = vectorizer.get_feature_names_out()

    print("ğŸ” Top 10 TF-IDF å…³é”®è¯ + å¯†åº¦ï¼š")
    for i in tfidf_values.argsort()[::-1][:10]:
        word = tfidf_words[i]
        tfidf_score = tfidf_values[i]
        freq = word_counts.get(word, 0)
        density = round((freq / total_words) * 100, 2)
        print(f"{word:<15} TF-IDF: {tfidf_score:.4f}  | å¯†åº¦: {density:.2f}%  | å‡ºç°æ¬¡æ•°: {freq}")

    top_word = tfidf_words[tfidf_values.argmax()]
    top_score = tfidf_values.max()
    total_score = tfidf_values.sum()

    warnings = []
    if top_score / total_score > tfidf_threshold:
        warnings.append(f"âš ï¸ TF-IDF å æ¯”è¿‡é«˜ï¼š'{top_word}'")

    for word in tfidf_words:
        freq = word_counts.get(word, 0)
        density = (freq / total_words) * 100
        if density > density_threshold:
            warnings.append(f"âš ï¸ å…³é”®è¯å¯†åº¦è¿‡é«˜ï¼š'{word}' å¯†åº¦={density:.2f}%")

    if warnings:
        print("\n".join(warnings))
        print("ğŸš¨ å¯èƒ½å­˜åœ¨å…³é”®è¯å †å è¡Œä¸ºï¼")
        return False
    else:
        print("âœ… æœªå‘ç°å¼‚å¸¸å…³é”®è¯å †å ã€‚")
        return True



